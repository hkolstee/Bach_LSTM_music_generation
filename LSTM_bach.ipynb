{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# progress bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# midi file creation\n",
    "from mingus.containers import Note\n",
    "from mingus.midi import fluidsynth\n",
    "from mingus.containers import Bar, Track, Composition\n",
    "from mingus.midi import midi_file_out\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(precision=4, suppress=True,linewidth=np.nan)\n",
    "# torch.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu if available (global variable for convenience)\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Side note: this is a notebook version of LSTM_bach.py which was used when training.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bach unfinished fugue \n",
    "**This project was one of the project suggestions for a course at my university (University of Groningen). I worked on the project with my teammates, but did not get (for me) satisfactory results, partly due to time constraints. We originally used a simple LSTM model (single small layer, implemented in tensorflow). I wanted to do it differently and started from scratch using PyTorch. All code in this repository folder is written by me. The description of the project is as follows:**\n",
    " \n",
    "Long time ago (1993) there was a time series prediction competition organized by the Santa Fe\n",
    "Institute which became rather famous in the community. The 6 data sets used for that competition have entered\n",
    "the folklore of the time series prediction community. Among the data sets, the last – and by far most difficult one\n",
    "– is an unfinished fugue written by Bach. The task was to use machine learning to complete the composition of\n",
    "this fugue. Achieving that would amount to finishing a genius' work… In retrospect, one can marvel at the\n",
    "audacity and innocence of the competition organizers; this task is of an unfathomable difficulty and far from\n",
    "being solved even today. But it is fun to try one's best and see what piece of artificial music art one can get with\n",
    "the machine learning tools that one masters. \n",
    "\n",
    "The unfinished fugue in question: ***Contrapunctus XIV: https://www.youtube.com/watch?v=JbM3VTIvOBk***\n",
    "\n",
    "An original source paper: ***santa_fe_competition.pdf***\n",
    "\n",
    "## Data:\n",
    "Given is a text file (input.txt) which consist of 4 sequences of integer numbers representing the 4 different voices of the fugue (voices = individual parts of the music piece played simultaniously). The integer numbers represent the pitch of the voice at that current point in time. When the pitch stays the same for multiple steps in time for a single voice, the pitch is supposed to be played for the entire duration. Every timestep is 1/16th of a bar.\n",
    "\n",
    "example (step 1251-1266):\n",
    "\n",
    "61\t55\t52\t47 <br>\n",
    "61\t55\t52\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "66\t54\t52\t47 <br>\n",
    "66\t54\t52\t47 <br>\n",
    "\n",
    "## Evaluation\n",
    "The train test split was configure as follows: the first 90 percent of the dataset was used as training data, while the last (which follows the train data) 10 percent was used as test data.  \n",
    "The data was standardized, making sure no information leaks occured. The scaler was fit on the train data and used to scale both the test and train data. Additionally, the sliding window input data which leaked time information from the train set into the test set were removed.\n",
    "\n",
    "We found the dataset to be too small, with the test data being not representative of the train data (features learned in train data did not generalize well to test data), resulting in models extracted using early stopping not generating good results.\n",
    "\n",
    "The lowest test losses occured quickly after the start of training, meaning the models overfit quickly. Even when using really small models (~8/16 hidden units, 1/2 conv layers with 8 channels, subsential dropouts) the models quickly overfit. Larger models sometimes achieved even lower test loss, but overfit thereafter quickly. It seemed that the current configuration of test/train split was not working as well as I would have liked. \n",
    "\n",
    "The next step I took was making a model with a large sliding window of 80 notes, with a large double LSTM layer of 256 units. My thought process was, as I couldn't rely as well on the test loss as I would've liked, I would train a substentially sized network and keep the complexity in check using dropout and weight decay/l2 regularization. The dropout was tuned by running different dropout frequencies and subjectively judging the generated music. I trained the model using google colab to a small training loss, and used this to generate bach-like music, which for the first time resulted in generated music that sounded like actual music. The outputs of different tests can be found in ***/output/...***.\n",
    "\n",
    "## Conclusion\n",
    "The dataset size and current train/test split configuration are not sufficient for good test evaluation.\n",
    "\n",
    "The first next step I thought of in this project would be try a different train/test config. My first idea is to take samples from different time point in the dataset instead of all at the end as the last part of the fugue is too different from the rest. A problem with this however, is that we need to prevent time leaks, and therefore, can't use the all the samples which have some of the notes of all those time windows taken for the test set. For example, if we have a sliding window of 80 time steps, which we use to predict the time step after that, we can't use any of the previous 79 sliding windows of a sliding window (sample) in the test set. All these 80 windows, which can be used to predict any of the time steps in this 80 time step window, will have to be removed. As the dataset is already small, this does not seem to be the best idea. \n",
    "\n",
    "Therefore, the logical next step making a well generalizing bach music generation model would be to find a different bigger dataset, which could be used to really learn patterns in the music that translate well to a test set. My solution to this was to tune dropout and regularization untill the model was able to generate music that continued the fugue which actually sounded like music, and most importantly, a possible ending to the fugue itself. In this, I paid extra attention to the first part of the generated music (continuing from where bach ended) as this is where overfit models struggled the most. The most overfit models (low dropout, low regularization) would catch itself in its mistakes later and produce decent results after sounding really bad, probably because they accidentally fall into some pattern that is in the training data and continue from here. If the model continues well where bach left off, I was content with how well it generalized (high dropout/regu models sounded worse after first part than overfit).\n",
    "\n",
    "The goal of this project was to finish the bach fugue, and eventhough the model is overfit compared to our test set, the generated music sounds pretty good. The model starts off really well, but gets in trouble after a while. However, the parts where you can here the model struggle still sounds greatly better than the models with the lowest test loss. Probably, there are small sections which are very similar to the training music after prediction by the model, but I am not sure if I have a problem with that in this context of the Santa Fe competition. \n",
    "\n",
    "Additionally, good musical knowledge + postprocessing/sampling of output of the lstm wouldn't hurt.\n",
    "\n",
    "## best ouput is **best_output.mp3**, which starts where bach left off."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (4 voices): (3804, 4)\n",
      "[[61. 55. 52. 47.]\n",
      " [61. 55. 52. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [66. 54. 52. 47.]\n",
      " [66. 54. 52. 47.]]\n"
     ]
    }
   ],
   "source": [
    "# load data, 4 voices of instruments\n",
    "voices = np.loadtxt(\"input.txt\")\n",
    "\n",
    "# remove starting silence, does not promote learning\n",
    "# data shape is (3816, 4) after\n",
    "voices = np.delete(voices, slice(8), axis=0)\n",
    "print(\"Data shape (4 voices):\", voices.shape)\n",
    "print(voices[1242:1258])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset\n",
    "### First, a one hot encode function for the target values, with an function that returns the location of the value in the set of uniques.'\n",
    "The target values are a one hot encoding of each voice target value, concatenated after each other. The loss is calculated with BCEWithLogitsLoss() which is often used for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find float index in unique float list of standard scaled array\n",
    "# works also for ints when not scaled\n",
    "def uniqueLocation(uniques, note):\n",
    "    for index, unique in enumerate(uniques):\n",
    "        if (math.isclose(unique, note, abs_tol=0.0001)):\n",
    "            return index\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns concatenated onehot encoding for each note \n",
    "def one_hot_encode(y: np.ndarray, voices: np.ndarray) -> np.ndarray:\n",
    "    # unique set of notes in the voice\n",
    "    unique_voice1 = np.unique(voices[:,0])\n",
    "    unique_voice2 = np.unique(voices[:,1])\n",
    "    unique_voice3 = np.unique(voices[:,2])\n",
    "    unique_voice4 = np.unique(voices[:,3])\n",
    "    total = len(unique_voice1) + len(unique_voice2) + len(unique_voice3) + len(unique_voice4)\n",
    "\n",
    "    # initialize return array\n",
    "    encoded = np.zeros((y.shape[0], total), dtype=np.float32)\n",
    "    \n",
    "    # one hot encode each note\n",
    "    for timestep, notes in enumerate(y):\n",
    "        for voice, note in enumerate(notes):\n",
    "            if (voice == 0):\n",
    "                # get location in uniques of current note\n",
    "                one_hot_location = uniqueLocation(unique_voice1, note)\n",
    "                encoded[timestep][one_hot_location] = 1\n",
    "            elif (voice == 1):\n",
    "                one_hot_location = uniqueLocation(unique_voice2, note)\n",
    "                encoded[timestep][one_hot_location + len(unique_voice1)] = 1\n",
    "            elif (voice == 2):\n",
    "                one_hot_location = uniqueLocation(unique_voice3, note)\n",
    "                encoded[timestep][one_hot_location + len(unique_voice1) + len(unique_voice2)] = 1\n",
    "            elif (voice == 3):\n",
    "                one_hot_location = uniqueLocation(unique_voice4, note)\n",
    "                encoded[timestep][one_hot_location + len(unique_voice1) + len(unique_voice2) + len(unique_voice3)] = 1\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset consisting of a window of notes, and target values of concatenated one hot encoded vectors of the voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_voices and all_voices used when creating a subset of all data for the current dataset (train/test)\n",
    "# necessary for one-hot encoding of test data\n",
    "class NotesDataset(Dataset):\n",
    "    def __init__(self, window_size: int, subset_voices:np.ndarray, all_voices: np.ndarray):\n",
    "        # nr of samples, and nr of voices\n",
    "        self.nr_samples = subset_voices.shape[0] - window_size\n",
    "        self.nr_voices = subset_voices.shape[1]\n",
    "\n",
    "        # initialize x data -> window_size amount of notes of 4 voices each per prediction\n",
    "        self.x = np.zeros((self.nr_samples, window_size, self.nr_voices), dtype=np.float32)\n",
    "        for i in range(self.x.shape[0]):\n",
    "            self.x[i] = subset_voices[i : i + window_size]\n",
    "\n",
    "        # initialize y data -> 4 following target notes per time window \n",
    "        self.y = np.zeros((self.nr_samples, self.nr_voices), dtype = np.float32)\n",
    "        for j in range(self.y.shape[0]):\n",
    "            self.y[j] = subset_voices[j + window_size]\n",
    "\n",
    "        # one hot encode target tensor\n",
    "        self.y = one_hot_encode(self.y, all_voices)\n",
    "\n",
    "        # create tensors\n",
    "        self.x = torch.from_numpy(self.x).to(device)\n",
    "        self.y = torch.from_numpy(self.y).to(device)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nr_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the test and train dataloader from the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test dataset based on window size where one window of timesteps\n",
    "#   will predict the subsequential single timestep\n",
    "# Data is created without any information leak between test/train (either scaling leak or time leak)\n",
    "def createTrainTestDataloaders(voices, split_size, window_size, batch_size):\n",
    "    # Train/test split\n",
    "    dataset_size = len(voices[:,])\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor((1 - split_size) * dataset_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "    # create split in data\n",
    "    train_voices = voices[train_indices, :]\n",
    "    test_voices = voices[test_indices, :]\n",
    "    \n",
    "    # scale both sets, using training data as fit (no leaks)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_voices)\n",
    "    train_voices = scaler.transform(train_voices)\n",
    "    all_voices = scaler.transform(voices)\n",
    "    \n",
    "    # create train dataset\n",
    "    train_dataset = NotesDataset(window_size, train_voices, all_voices)\n",
    "\n",
    "    # create train dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size)\n",
    "\n",
    "    # Do the same for test set \n",
    "    if (split_size > 0):\n",
    "        # scale test set\n",
    "        test_voices = scaler.transform(test_voices)\n",
    "        # create test dataset\n",
    "        test_dataset = NotesDataset(window_size, test_voices, all_voices)\n",
    "        # create test dataloader\n",
    "        test_loader = DataLoader(test_dataset, batch_size)\n",
    "    else:\n",
    "        test_loader = None\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "The model consists of 3 convolutional layers followed by a double LSTM layer. Dropout frequency has been tuned for best results, along with weight decay (L2 regularization) to keep the model from becoming too complex while still learning meaningful patterns in the limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model with three conv layers\n",
    "# The model can be set to stateful, meaning the internal hidden state and cell state is passed\n",
    "#   into the model each batch and reset once per epoch.\n",
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, batch_size, channels):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_channels = channels\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.25) \n",
    "\n",
    "        # first conv layer\n",
    "        padding = 1\n",
    "        kernel_conv2d = 2\n",
    "        c_out = channels\n",
    "        lstm_input_size = input_size - kernel_conv2d + (2 * padding) + 1\n",
    "        self.conv2d_1 = nn.Conv2d(1, c_out, kernel_size = kernel_conv2d, padding = padding)\n",
    "\n",
    "        # # second conv layer\n",
    "        padding = 1\n",
    "        kernel_conv2d += 1\n",
    "        c_out2 = c_out * 2\n",
    "        lstm_input_size = lstm_input_size - kernel_conv2d + (2 * padding) + 1\n",
    "        self.conv2d_2 = nn.Conv2d(c_out, c_out2, kernel_size = kernel_conv2d, padding = padding)\n",
    "\n",
    "        # third conv layer\n",
    "        padding = 1\n",
    "        kernel_conv2d += 1\n",
    "        c_out3 = c_out2 * 2\n",
    "        lstm_input_size = lstm_input_size - kernel_conv2d + (2 * padding) + 1\n",
    "        self.conv2d_3 = nn.Conv2d(c_out2, c_out3, kernel_size = kernel_conv2d, padding = padding)\n",
    "\n",
    "        self.lstm = nn.LSTM(c_out3 * lstm_input_size, hidden_size, num_layers, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        print(\"LSTM initialized with {} input size, {} hidden layer size, {} number of LSTM layers, and an output size of {}\".format(input_size, hidden_size, num_layers, output_size))\n",
    "        # reset states in case of stateless use\n",
    "        self.reset_states(batch_size)\n",
    "\n",
    "    # reset hidden state and cell state, should be before each new sequence\n",
    "    #   In our problem: every epoch, as it is one long sequence\n",
    "    def reset_states(self, batch_size):\n",
    "    # def reset_states(self):\n",
    "        # hidden state and cell state for LSTM \n",
    "        self.hn = torch.zeros(self.num_layers,  batch_size, self.hidden_size).to(device)\n",
    "        self.cn = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, input, stateful):\n",
    "        # conv layer wont take it right now as it seems to have batch_size number of channels \n",
    "        # [batch_size,window_size,4]->[batch_size,1,window_size,4]\n",
    "        input = input.unsqueeze(1)\n",
    "\n",
    "        # pass through first conv layer\n",
    "        out = self.conv2d_1(input)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # # dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # # pass through second conv layer\n",
    "        out = self.conv2d_2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # # dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # # # pass through third conv layer\n",
    "        out = self.conv2d_3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # # reshape for the lstm\n",
    "        out = out.view(input.size(0), out.size(2), -1)\n",
    "\n",
    "        # simple forward function\n",
    "        # stateful = keep hidden states entire sequence length\n",
    "        if stateful:\n",
    "            # for last batch which might not be the same shape\n",
    "            if (input.size(0) != self.hn.size(1)):\n",
    "                self.reset_states(input.size(0))\n",
    "              \n",
    "            # lstm layer\n",
    "            out, (self.hn, self.cn) = self.lstm(out, (self.hn.detach(), self.cn.detach())) \n",
    "            # linear output layer\n",
    "            out = self.linear(out[:,-1,:])\n",
    "        else:\n",
    "            # initiaze hidden and cell states\n",
    "            hn = torch.zeros(self.num_layers,  input.size(0), self.hidden_size).to(device)\n",
    "            cn = torch.zeros(self.num_layers, input.size(0), self.hidden_size).to(device)\n",
    "            # lstm layer\n",
    "            out, (hn, cn) = self.lstm(out, (hn, cn))\n",
    "            # linear output layer\n",
    "            out = self.linear(out[:,-1,:])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training(model, train_loader:DataLoader, test_loader:DataLoader, nr_epochs, optimizer, loss_func, scheduler, stateful, writer):\n",
    "    # lowest train/test loss\n",
    "    lowest_train_loss = np.inf\n",
    "    lowest_test_loss = np.inf\n",
    "    \n",
    "    # training loop\n",
    "    for epoch in range(1, nr_epochs):\n",
    "        # reset running loss\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        # reset lstm hidden and cell state (stateful lstm = reset states once per sequence)\n",
    "        # if not, reset automatically each forward call\n",
    "        if stateful:\n",
    "            model.reset_states(train_loader.batch_size)\n",
    "        \n",
    "        # train loop\n",
    "        model.train()\n",
    "        for i, (inputs, labels) in enumerate(tqdm(train_loader)):\n",
    "            # reset gradient function of weights\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            prediction = model(inputs, stateful)\n",
    "            # calculate loss\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # backward, retain_graph = True needed for hidden lstm states\n",
    "            loss.backward(retain_graph=True)\n",
    "            # step\n",
    "            optimizer.step()\n",
    "            # add to running loss\n",
    "            running_loss_train += loss.item()\n",
    "        # learning rate scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # calc running loss\n",
    "        train_loss = running_loss_train/len(train_loader)\n",
    "\n",
    "        # add loss to tensorboard\n",
    "        writer.add_scalar(\"Running train loss\", train_loss, epoch)        \n",
    "\n",
    "        # check if lowest loss\n",
    "        if (train_loss < lowest_train_loss):\n",
    "            lowest_train_loss = train_loss\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), \"models/model\" + str(train_loader.dataset.x.shape[1]) + str(model.hidden_size) + str(model.conv_channels) + \".pth\")\n",
    "\n",
    "        # Test evaluation\n",
    "        if (test_loader):\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for j, (inputs, labels) in enumerate(test_loader):\n",
    "                    # forward pass\n",
    "                    prediction = model(inputs, stateful)\n",
    "                    # calculate loss\n",
    "                    test_loss = loss_func(prediction, labels)\n",
    "                    # add to running loss\n",
    "\n",
    "            # calc running loss\n",
    "            test_loss = running_loss_test/len(test_loader)\n",
    "\n",
    "            # add test loss to tensorboard\n",
    "            writer.add_scalar(\"Running test loss\", test_loss, epoch)\n",
    "\n",
    "            # if lowest till now, save model (checkpointing)\n",
    "            if (test_loss < lowest_test_loss):\n",
    "                lowest_test_loss = test_loss\n",
    "                torch.save(model.state_dict(), \"models/model\" + str(train_loader.dataset.x.shape[1]) + str(model.hidden_size) + str(model.conv_channels) + \"test\" + \".pth\")\n",
    "\n",
    "        # print training running loss and add to tensorboard\n",
    "        print(\"Epoch:\", epoch, \"  Train loss:\", train_loss,\n",
    "                                \", Test loss:\", test_loss if test_loader else \"Not available\")\n",
    "    \n",
    "    return lowest_train_loss, lowest_test_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamers\n",
    "The hyperparameters dictionary acts as a grid search if multiple values for the parameters are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size for training network\n",
    "batch_size = 32\n",
    "\n",
    "# split size of test/train data\n",
    "split_size = 0.0\n",
    "\n",
    "# hyperparameters for fine-tuning\n",
    "    # window_size = sliding window on time-sequence data for input\n",
    "    # hidden_size = hidden units of lstm layer(s)\n",
    "    # conv_channels = number of channels in the first conv layer (multiplied by 2 every next layer)\n",
    "    # nr_layers = number of lstm layers stacked after each other\n",
    "hyperparams = dict(\n",
    "    window_size = [80],\n",
    "    hidden_size = [256],\n",
    "    conv_channels = [8],\n",
    "    nr_layers = [2]\n",
    ")\n",
    "hyperparam_values = [value for value in hyperparams.values()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New run window/hidden/channels/batch_size: 80 / 256 / 8 / 32\n",
      "Input size: torch.Size([32, 80, 4]) - Output size: torch.Size([32, 98]) - TRAIN batches: 117 - TEST batches: Not available\n",
      "LSTM initialized with 4 input size, 256 hidden layer size, 2 number of LSTM layers, and an output size of 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:07<00:00,  1.09s/it]\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1   Train loss: 0.1905215892654199 , Test loss: Not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [02:30<00:00,  1.28s/it]\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2   Train loss: 0.15328861091636184 , Test loss: Not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [03:03<00:00,  1.57s/it]\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3   Train loss: 0.15322115501532188 , Test loss: Not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [01:16<00:00,  1.54it/s]\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4   Train loss: 0.15321783638662761 , Test loss: Not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [01:49<00:00,  1.07it/s]\n",
      "  0%|          | 0/117 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5   Train loss: 0.1531626820946351 , Test loss: Not available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 81/117 [01:18<00:34,  1.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[136], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m1500\u001b[39m\n\u001b[1;32m     35\u001b[0m stateful \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m lowest_train_loss, lowest_test_loss \u001b[39m=\u001b[39m training(lstm_model, train_loader, test_loader, epochs, optimizer, loss_func, scheduler, stateful, writer)\n\u001b[1;32m     38\u001b[0m \u001b[39m# save hparams along with lowest train/test losses\u001b[39;00m\n\u001b[1;32m     39\u001b[0m writer\u001b[39m.\u001b[39madd_hparams(\n\u001b[1;32m     40\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mwindow_size\u001b[39m\u001b[39m\"\u001b[39m: window_size, \u001b[39m\"\u001b[39m\u001b[39mhidden_size\u001b[39m\u001b[39m\"\u001b[39m: hidden_size, \u001b[39m\"\u001b[39m\u001b[39mconv_channels\u001b[39m\u001b[39m\"\u001b[39m: conv_channels},\n\u001b[1;32m     41\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mMinTrainLoss\u001b[39m\u001b[39m\"\u001b[39m: lowest_train_loss, \u001b[39m\"\u001b[39m\u001b[39mMinTestLoss\u001b[39m\u001b[39m\"\u001b[39m: lowest_test_loss},\n\u001b[1;32m     42\u001b[0m )\n",
      "Cell \u001b[0;32mIn[134], line 26\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(model, train_loader, test_loader, nr_epochs, optimizer, loss_func, scheduler, stateful, writer)\u001b[0m\n\u001b[1;32m     24\u001b[0m loss \u001b[39m=\u001b[39m loss_func(prediction, labels)\n\u001b[1;32m     25\u001b[0m \u001b[39m# backward, retain_graph = True needed for hidden lstm states\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# step\u001b[39;00m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through different combinations of the hyperparameters\n",
    "for run_id, (window_size, hidden_size, conv_channels, nr_layers) in enumerate(product(*hyperparam_values)):\n",
    "        # tensorboard summary writer\n",
    "        writer = SummaryWriter(f'runs/window_size={window_size} hidden_size={hidden_size} conv_channels={conv_channels}')\n",
    "        \n",
    "        # Split data in train and test, scale, create datasets and create dataloaders\n",
    "        train_loader, test_loader = createTrainTestDataloaders(voices, split_size, window_size, batch_size)\n",
    "\n",
    "        # some informational print statements\n",
    "        print(\"\\nNew run window/hidden/channels/batch_size:\", window_size, \"/\", hidden_size, \"/\", conv_channels, \"/\", batch_size)\n",
    "        features, labels = next(iter(train_loader))\n",
    "        print(\"Input size:\", features.size(), \n",
    "            \"- Output size:\", labels.size(), \n",
    "            \"- TRAIN batches:\", len(train_loader), \n",
    "            \"- TEST batches:\", len(test_loader) if test_loader else \"Not available\")\n",
    "        # Input/output dimensions\n",
    "        input_size = voices.shape[1]\n",
    "        output_size = labels.size(1)\n",
    "\n",
    "        # create model\n",
    "        lstm_model = LSTM_model(input_size, output_size, hidden_size, nr_layers, batch_size, conv_channels)\n",
    "\n",
    "        # loss function and optimizer\n",
    "        #   multi lable one hot encoded prediction only works with BCEwithlogitloss\n",
    "        loss_func = nn.BCEWithLogitsLoss()\n",
    "        # AdamW = Adam with fixed weight decay (weight decay performed after controlling parameter-wise step size)\n",
    "        optimizer = optim.AdamW(lstm_model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "        scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=1100)\n",
    "        \n",
    "        # to gpu if possible\n",
    "        lstm_model = lstm_model.to(device)\n",
    "        \n",
    "        # training loop\n",
    "        epochs = 1500\n",
    "        stateful = True\n",
    "        lowest_train_loss, lowest_test_loss = training(lstm_model, train_loader, test_loader, epochs, optimizer, loss_func, scheduler, stateful, writer)\n",
    "        \n",
    "        # save hparams along with lowest train/test losses\n",
    "        writer.add_hparams(\n",
    "            {\"window_size\": window_size, \"hidden_size\": hidden_size, \"conv_channels\": conv_channels},\n",
    "            {\"MinTrainLoss\": lowest_train_loss, \"MinTestLoss\": lowest_test_loss},\n",
    "        )\n",
    "        # tb writer flush\n",
    "        writer.flush()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate music using one of the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to predict x next notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNextNotes(input, steps, lstm_model, voices, scaler):\n",
    "    # predicted notes\n",
    "    predicted_notes = np.zeros((1,4))\n",
    "\n",
    "    # all unique notes for each voice\n",
    "    unique_voice1 = np.unique(voices[:,0])\n",
    "    unique_voice2 = np.unique(voices[:,1])\n",
    "    unique_voice3 = np.unique(voices[:,2])\n",
    "    unique_voice4 = np.unique(voices[:,3])\n",
    "    one_hot_values = np.concatenate((unique_voice1, unique_voice2, unique_voice3, unique_voice4))\n",
    "\n",
    "    # BCEwithLogitLoss uses sigmoid when calculating loss, but we need to pass through\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # prepare input\n",
    "    input = torch.tensor(input, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(steps):\n",
    "            # print(input.shape)\n",
    "            output = lstm_model(input, stateful=False)\n",
    "            output = sigmoid(output)\n",
    "            # print(output)\n",
    "            output = output.detach().numpy().squeeze()\n",
    "\n",
    "            # get the indices with highest value from model forward output\n",
    "            note_voice1 = np.argmax(output[:len(unique_voice1)])\n",
    "            note_voice2 = np.argmax(output[len(unique_voice1) : len(unique_voice1) + len(unique_voice2)])\n",
    "            note_voice3 = np.argmax(output[len(unique_voice1) + len(unique_voice2) : len(unique_voice1) + len(unique_voice2) + len(unique_voice3)])\n",
    "            note_voice4 = np.argmax(output[-len(unique_voice4):])\n",
    "            # print(note_voice1, note_voice2, note_voice3, note_voice4)\n",
    "\n",
    "            # get notes\n",
    "            note_voice1 = one_hot_values[note_voice1]\n",
    "            note_voice2 = one_hot_values[len(unique_voice1) + note_voice2]\n",
    "            note_voice3 = one_hot_values[len(unique_voice1) + len(unique_voice2) + note_voice3]\n",
    "            note_voice4 = one_hot_values[len(unique_voice1) + len(unique_voice2) + len(unique_voice3) + note_voice4]\n",
    "\n",
    "            # add to array and inverse scale\n",
    "            next_notes = np.array([note_voice1, note_voice2, note_voice3, note_voice4])\n",
    "            next_notes_invscaled = scaler.inverse_transform(next_notes.reshape(1, -1))\n",
    "            # print(next_notes_invscaled)\n",
    "            predicted_notes = np.concatenate((predicted_notes, next_notes_invscaled), axis = 0)\n",
    "            # print(predicted_notes)\n",
    "\n",
    "            # change input\n",
    "            # drop oldest notes\n",
    "            input = input[0][1:]\n",
    "            # concat predicted notes\n",
    "            input = torch.cat((input, torch.Tensor(next_notes).unsqueeze(0)))\n",
    "            input = input.unsqueeze(0)\n",
    "\n",
    "    return(predicted_notes.astype(np.int32)[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and generate new notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'conv_channels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# initialize model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m LSTM_model(input_size \u001b[39m=\u001b[39;49m \u001b[39m4\u001b[39;49m, output_size \u001b[39m=\u001b[39;49m \u001b[39m98\u001b[39;49m, hidden_size \u001b[39m=\u001b[39;49m \u001b[39m256\u001b[39;49m, num_layers \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, batch_size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m, conv_channels \u001b[39m=\u001b[39;49m \u001b[39m8\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# cremove map location\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mmodels/LSTM_medlow_dropout.pth\u001b[39m\u001b[39m\"\u001b[39m, map_location\u001b[39m=\u001b[39mdevice))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'conv_channels'"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = LSTM_model(input_size = 4, output_size = 98, hidden_size = 256, num_layers = 2, batch_size = 32, channels = 8)\n",
    "# load model file\n",
    "model.load_state_dict(torch.load(\"models/LSTM_medlow_dropout.pth\", map_location=device))\n",
    "\n",
    "# load data, 4 voices of instruments\n",
    "voices = np.loadtxt(\"input.txt\")\n",
    "\n",
    "# Train/test split (needed for correct scaling of new data)\n",
    "dataset_size = len(voices[:,])\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor((1 - split_size) * dataset_size))\n",
    "train_indices = indices[:split]\n",
    "# create split in data\n",
    "train_voices = voices[train_indices, :]\n",
    "\n",
    "# fit the scaler to the train data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_voices)\n",
    "# scale voices\n",
    "voices = scaler.transform(voices)\n",
    "train_voices = scaler.transform(train_voices)\n",
    "\n",
    "# take last sliding window in data and infer from there\n",
    "input = train_voices[-window_size:]\n",
    "steps = 1500\n",
    "new_music = predictNextNotes(input, steps, model, voices, scaler)\n",
    "\n",
    "# save new music\n",
    "np.savetxt(fname = \"output/output.txt\", X = new_music, fmt = \"%d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a midi files from generated music (output and complete fugue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fluidsynth with soundfont\n",
    "fluidsynth.init(\"soundfonts/040_Florestan_String_Qraurtet.sf2\", \"oss\")\n",
    "\n",
    "# load original music\n",
    "original = np.loadtxt(\"input.txt\")\n",
    "\n",
    "# load network output music\n",
    "output = np.loadtxt(\"output/output.txt\")\n",
    "\n",
    "# concatenate\n",
    "complete = np.concatenate((original, output), axis = 0)\n",
    "\n",
    "# create midi for output and comlete (original + output)\n",
    "for run, voices in enumerate([output, complete]):\n",
    "    # create 4 tracks for the 4 voices\n",
    "    encoded_voices = [Track(), Track(), Track(), Track()] \n",
    "    \n",
    "    # loop through the generated voices\n",
    "    for i, notes in enumerate([voices[:,0], voices[:,1], voices[:,2], voices[:,3]]):\n",
    "        # initialize as impossible note\n",
    "        last_note = -1\n",
    "        count = 1\n",
    "        for j, note in enumerate(notes):\n",
    "            if note:\n",
    "                if ((note == last_note) or (j == 0)):\n",
    "                    # same note as previous note\n",
    "                    count += 1\n",
    "                    last_note = note\n",
    "                    \n",
    "                    if (j + count > len(notes)):\n",
    "                        # current note reaches end of file\n",
    "                        n = Note()\n",
    "                        n.from_int(int(last_note))\n",
    "                        b = Bar()\n",
    "                        b.place_notes(n, 16/count)\n",
    "                        encoded_voices[i].add_bar(b)\n",
    "                else:\n",
    "                    # different note encountered\n",
    "                    # add previous note with its duration to track\n",
    "                    n = Note()\n",
    "                    n.from_int(int(last_note))\n",
    "                    b = Bar()\n",
    "                    \n",
    "                    # 8 should be 1/2 -> 2\n",
    "                    # 16 should be 1 -> 1\n",
    "                    # 32 should be 2 -> 0.5\n",
    "                    b.place_notes(n, duration = 16/count)\n",
    "                    encoded_voices[i].add_bar(b)\n",
    "                    \n",
    "                    # reset\n",
    "                    count = 1\n",
    "                    last_note = note\n",
    "            else:\n",
    "                # current note = 0, means a pause (silence)\n",
    "                b = Bar()\n",
    "                b.place_rest(16)\n",
    "                encoded_voices[i].add_bar(b)\n",
    "\n",
    "    output_composition = Composition()\n",
    "    output_composition.add_track(encoded_voices[0])\n",
    "    output_composition.add_track(encoded_voices[1])\n",
    "    output_composition.add_track(encoded_voices[2])\n",
    "    output_composition.add_track(encoded_voices[3])\n",
    "\n",
    "    if (run == 0):\n",
    "        midi_file_out.write_Composition(\"output/output.midi\", output_composition)\n",
    "    else:\n",
    "        midi_file_out.write_Composition(\"output/complete.midi\", output_composition)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mp3\n",
    "For Linux.  \n",
    "windows -> remove %%bash, add !before each fluidsynth call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'soundfonts/040_Florestan_String_Qaurtet.sf2' not a SoundFont or MIDI file or error occurred identifying it.\n",
      "Parameter 'output/output.midi' not a SoundFont or MIDI file or error occurred identifying it.\n",
      "No midi file specified!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'soundfonts/040_Florestan_String_Qaurtet.sf2' not a SoundFont or MIDI file or error occurred identifying it.\n",
      "Parameter 'output/complete.midi' not a SoundFont or MIDI file or error occurred identifying it.\n",
      "No midi file specified!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FluidSynth runtime version 2.1.1\n",
      "Copyright (C) 2000-2020 Peter Hanappe and others.\n",
      "Distributed under the LGPL license.\n",
      "SoundFont(R) is a registered trademark of E-mu Systems, Inc.\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'fluidsynth \"soundfonts/040_Florestan_String_Qaurtet.sf2\" -F \"output/output.mp3\" \"output/output.midi\"\\nfluidsynth \"soundfonts/040_Florestan_String_Qaurtet.sf2\" -F \"output/complete.mp3\" \"output/complete.midi\"\\n'' returned non-zero exit status 255.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[139], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mbash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mfluidsynth \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msoundfonts/040_Florestan_String_Qaurtet.sf2\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m -F \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput/output.mp3\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput/output.midi\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mfluidsynth \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msoundfonts/040_Florestan_String_Qaurtet.sf2\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m -F \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput/complete.mp3\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39moutput/complete.midi\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2417\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2415\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2416\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2417\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2418\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'fluidsynth \"soundfonts/040_Florestan_String_Qaurtet.sf2\" -F \"output/output.mp3\" \"output/output.midi\"\\nfluidsynth \"soundfonts/040_Florestan_String_Qaurtet.sf2\" -F \"output/complete.mp3\" \"output/complete.midi\"\\n'' returned non-zero exit status 255."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "timidity output/output.midi -Ow -o - | ffmpeg -i - -acodec libmp3lame -ab 320k output/output.mp3\n",
    "timidity output/complete.midi -Ow -o - | ffmpeg -i - -acodec libmp3lame -ab 320k output/complete3.mp3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
