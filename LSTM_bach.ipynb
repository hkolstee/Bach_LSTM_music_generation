{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# progress bar\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# midi file creation\n",
    "from mingus.containers import Note\n",
    "from mingus.midi import fluidsynth\n",
    "from mingus.containers import Bar, Track, Composition\n",
    "from mingus.midi import midi_file_out\n",
    "\n",
    "# np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(precision=4, suppress=True,linewidth=np.nan)\n",
    "# torch.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu if available (global variable for convenience)\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bach unfinished fugue \n",
    "**This project was one of the project suggestions for a course at my university (University of Groningen). I worked on the project with my teammates, but did not get (for me) satisfactory results, partly due to time constraints. We originally used a simple LSTM model (single small layer, implemented in tensorflow). I wanted to do it differently and started from scratch using PyTorch. All code in this repository folder is written by me. The description of the project is as follows:**\n",
    " \n",
    "Long time ago (1993) there was a time series prediction competition organized by the Santa Fe\n",
    "Institute which became rather famous in the community. The 6 data sets used for that competition have entered\n",
    "the folklore of the time series prediction community. Among the data sets, the last – and by far most difficult one\n",
    "– is an unfinished fugue written by Bach. The task was to use machine learning to complete the composition of\n",
    "this fugue. Achieving that would amount to finishing a genius' work… In retrospect, one can marvel at the\n",
    "audacity and innocence of the competition organizers; this task is of an unfathomable difficulty and far from\n",
    "being solved even today. But it is fun to try one's best and see what piece of artificial music art one can get with\n",
    "the machine learning tools that one masters. \n",
    "\n",
    "The unfinished fugue in question: ***Contrapunctus XIV: https://www.youtube.com/watch?v=JbM3VTIvOBk***\n",
    "\n",
    "An original source paper: ***santa_fe_competition.pdf***\n",
    "\n",
    "## Data:\n",
    "Given is a text file (input.txt) which consist of 4 sequences of integer numbers representing the 4 different voices of the fugue (voices = individual parts of the music piece played simultaniously). The integer numbers represent the pitch of the voice at that current point in time. When the pitch stays the same for multiple steps in time for a single voice, the pitch is supposed to be played for the entire duration. Every timestep is 1/16th of a bar.\n",
    "\n",
    "example (step 1251-1266):\n",
    "\n",
    "61\t55\t52\t47 <br>\n",
    "61\t55\t52\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "61\t55\t49\t47 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t54\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "73\t54\t52\t46 <br>\n",
    "66\t54\t52\t47 <br>\n",
    "66\t54\t52\t47 <br>\n",
    "\n",
    "## Evaluation\n",
    "The train test split was configure as follows: the first 90 percent of the dataset was used as training data, while the last (which follows the train data) 10 percent was used as test data.  \n",
    "The data was standardized, making sure no information leaks occured. The scaler was fit on the train data and used to scale both the test and train data. Additionally, the sliding window input data which leaked time information from the train set into the test set were removed.\n",
    "\n",
    "We found the dataset to be too small, with the test data being not representative of the train data (features learned in train data did not generalize well to test data), resulting in models extracted using early stopping not generating good results.\n",
    "\n",
    "The lowest test losses occured quickly after the start of training, meaning the models overfit quickly. Even when using really small models (~8/16 hidden units, 1/2 conv layers with 8 channels, subsential dropouts) the models quickly overfit. Larger models sometimes achieved even lower test loss, but overfit thereafter quickly. It seemed that the current configuration of test/train split was not working as well as I would have liked. \n",
    "\n",
    "The next step I took was making a models with a large sliding window of 80 notes, and playing around with a large double LSTM layers of 128 and 256 units. My thought process was, as I couldn't rely as well on the test loss as I would've liked, I would train a substentially sized network (to prevent underspecification) and keep the complexity in check using dropout and weight decay/l2 regularization. \n",
    "\n",
    "I also found a paper of which the experimental results find that the number of parameters is not a good indication of whether a model will overfit: \n",
    "\n",
    "<span style=\"color:yellow\"> *\"Zhang, C., Bengio, S., Hardt, M., Recht, B., & Vinyals, O. (2016). Understanding deep learning requires rethinking generalization. ArXiv. https://arxiv.org/abs/1611.03530\"* </span>\n",
    "\n",
    "regularization was tuned by choosing different amounts of dropout frequency between the LSTM layers and different weight decay parameter values. Additionally, dropout between the convolutional layers was added. This convolutional dropout was not tuned (kept at 0.1) and based on the paper: \n",
    "\n",
    "<span style=\"color:yellow\"> *\"Park, S., Kwak, N. (2017). Analysis on the Dropout Effect in Convolutional Neural Networks. In: Lai, SH., Lepetit, V., Nishino, K., Sato, Y. (eds) Computer Vision – ACCV 2016. ACCV 2016. Lecture Notes in Computer Science(), vol 10112. Springer, Cham. https://doi.org/10.1007/978-3-319-54184-6_12\"* </span>\n",
    "\n",
    "Subsequently, I subjectively judging the generated music by listening to it. Mostly, I listened paid attention to the point where the model continued where bach left off, before it could fall back into patterns it had learned, so to judge the generaliztion capacity of the model. I trained the model using google colab to a small training loss, and used this to generate bach-like music, which for the first time resulted in generated music that sounded like actual music. The outputs of different tests can be found in ***/output/...***.\n",
    "\n",
    "## Conclusion\n",
    "The dataset size and current train/test split configuration are not sufficient for good test evaluation.\n",
    "\n",
    "The first next step I thought of in this project would be try a different train/test config. My first idea is to take samples from different time point in the dataset instead of all at the end as the last part of the fugue is too different from the rest. A problem with this however, is that we need to prevent time leaks, and therefore, can't use the all the samples which have some of the notes of all those time windows taken for the test set. For example, if we have a sliding window of 80 time steps, which we use to predict the time step after that, we can't use any of the previous 79 sliding windows of a sliding window (sample) in the test set. All these 80 windows, which can be used to predict any of the time steps in this 80 time step window, will have to be removed. As the dataset is already small, this does not seem to be the best idea. \n",
    "\n",
    "Therefore, the logical next step making a well generalizing bach music generation model would be to find a different bigger dataset, which could be used to really learn patterns in the music that translate well to a test set. My solution to this was to tune dropout and regularization untill the model was able to generate music that continued the fugue which actually sounded like music, and most importantly, a possible ending to the fugue itself. In this, I paid extra attention to the first part of the generated music (continuing from where bach ended) as this is where overfit models struggled the most. The most overfit models (low dropout, low regularization) would catch itself in its mistakes later and produce decent results after sounding really bad, probably because they accidentally fall into some pattern that is in the training data and continue from here. If the model continues well where bach left off, I was content with how well it generalized (high dropout/regu models sounded worse after first part than overfit).\n",
    "\n",
    "The goal of this project was to finish the bach fugue, and eventhough the model is overfit compared to our test set, the generated music sounds pretty good. The model starts off really well, but gets in trouble after a while. However, the parts where you can here the model struggle still sounds greatly better than the models with the lowest test loss. Probably, there are small sections which are very similar to the training music after prediction by the model, but I am not sure if I have a problem with that in this context of the Santa Fe competition. \n",
    "\n",
    "Additionally, good musical knowledge + postprocessing/sampling of output of the lstm wouldn't hurt.\n",
    "\n",
    "## best ouput is **best_output.mp3**, which starts where bach left off."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape (4 voices): (3804, 4)\n",
      "[[61. 55. 52. 47.]\n",
      " [61. 55. 52. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [61. 55. 49. 47.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 54. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [73. 54. 52. 46.]\n",
      " [66. 54. 52. 47.]\n",
      " [66. 54. 52. 47.]]\n"
     ]
    }
   ],
   "source": [
    "# load data, 4 voices of instruments\n",
    "voices = np.loadtxt(\"input.txt\")\n",
    "\n",
    "# remove starting silence, does not promote learning\n",
    "# data shape is (3816, 4) after\n",
    "voices = np.delete(voices, slice(8), axis=0)\n",
    "print(\"Data shape (4 voices):\", voices.shape)\n",
    "print(voices[1242:1258])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom dataset\n",
    "### First, a one hot encode function for the target values, with an function that returns the location of the value in the set of uniques.'\n",
    "The target values are a one hot encoding of each voice target value, concatenated after each other. The loss is calculated with BCEWithLogitsLoss() which is often used for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find float index in unique float list of standard scaled array\n",
    "# works also for ints when not scaled\n",
    "def uniqueLocation(uniques, note):\n",
    "    for index, unique in enumerate(uniques):\n",
    "        if (math.isclose(unique, note, abs_tol=0.0001)):\n",
    "            return index\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns concatenated onehot encoding for each note \n",
    "def one_hot_encode(y: np.ndarray, voices: np.ndarray) -> np.ndarray:\n",
    "    # unique set of notes in the voice\n",
    "    unique_voice1 = np.unique(voices[:,0])\n",
    "    unique_voice2 = np.unique(voices[:,1])\n",
    "    unique_voice3 = np.unique(voices[:,2])\n",
    "    unique_voice4 = np.unique(voices[:,3])\n",
    "    total = len(unique_voice1) + len(unique_voice2) + len(unique_voice3) + len(unique_voice4)\n",
    "\n",
    "    # initialize return array\n",
    "    encoded = np.zeros((y.shape[0], total), dtype=np.float32)\n",
    "    \n",
    "    # one hot encode each note\n",
    "    for timestep, notes in enumerate(y):\n",
    "        for voice, note in enumerate(notes):\n",
    "            if (voice == 0):\n",
    "                # get location in uniques of current note\n",
    "                one_hot_location = uniqueLocation(unique_voice1, note)\n",
    "                encoded[timestep][one_hot_location] = 1\n",
    "            elif (voice == 1):\n",
    "                one_hot_location = uniqueLocation(unique_voice2, note)\n",
    "                encoded[timestep][one_hot_location + len(unique_voice1)] = 1\n",
    "            elif (voice == 2):\n",
    "                one_hot_location = uniqueLocation(unique_voice3, note)\n",
    "                encoded[timestep][one_hot_location + len(unique_voice1) + len(unique_voice2)] = 1\n",
    "            elif (voice == 3):\n",
    "                one_hot_location = uniqueLocation(unique_voice4, note)\n",
    "                encoded[timestep][one_hot_location + len(unique_voice1) + len(unique_voice2) + len(unique_voice3)] = 1\n",
    "\n",
    "    return encoded"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset consisting of a window of notes, and target values of concatenated one hot encoded vectors of the voices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_voices and all_voices used when creating a subset of all data for the current dataset (train/test)\n",
    "# necessary for one-hot encoding of test data\n",
    "class NotesDataset(Dataset):\n",
    "    def __init__(self, window_size: int, subset_voices:np.ndarray, all_voices: np.ndarray):\n",
    "        # nr of samples, and nr of voices\n",
    "        self.nr_samples = subset_voices.shape[0] - window_size\n",
    "        self.nr_voices = subset_voices.shape[1]\n",
    "\n",
    "        # initialize x data -> window_size amount of notes of 4 voices each per prediction\n",
    "        self.x = np.zeros((self.nr_samples, window_size, self.nr_voices), dtype=np.float32)\n",
    "        for i in range(self.x.shape[0]):\n",
    "            self.x[i] = subset_voices[i : i + window_size]\n",
    "\n",
    "        # initialize y data -> 4 following target notes per time window \n",
    "        self.y = np.zeros((self.nr_samples, self.nr_voices), dtype = np.float32)\n",
    "        for j in range(self.y.shape[0]):\n",
    "            self.y[j] = subset_voices[j + window_size]\n",
    "\n",
    "        # one hot encode target tensor\n",
    "        self.y = one_hot_encode(self.y, all_voices)\n",
    "\n",
    "        # create tensors\n",
    "        self.x = torch.from_numpy(self.x).to(device)\n",
    "        self.y = torch.from_numpy(self.y).to(device)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nr_samples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create the test and train dataloader from the custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test dataset based on window size where one window of timesteps\n",
    "#   will predict the subsequential single timestep\n",
    "# Data is created without any information leak between test/train (either scaling leak or time leak)\n",
    "def createTrainTestDataloaders(voices, split_size, window_size, batch_size):\n",
    "    # Train/test split\n",
    "    dataset_size = len(voices[:,])\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor((1 - split_size) * dataset_size))\n",
    "    train_indices, test_indices = indices[:split], indices[split:]\n",
    "\n",
    "    # create split in data\n",
    "    train_voices = voices[train_indices, :]\n",
    "    test_voices = voices[test_indices, :]\n",
    "    \n",
    "    # scale both sets, using training data as fit (no leaks)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_voices)\n",
    "    train_voices = scaler.transform(train_voices)\n",
    "    all_voices = scaler.transform(voices)\n",
    "    \n",
    "    # create train dataset\n",
    "    train_dataset = NotesDataset(window_size, train_voices, all_voices)\n",
    "\n",
    "    # create train dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size)\n",
    "\n",
    "    # Do the same for test set \n",
    "    if (split_size > 0):\n",
    "        # scale test set\n",
    "        test_voices = scaler.transform(test_voices)\n",
    "        # create test dataset\n",
    "        test_dataset = NotesDataset(window_size, test_voices, all_voices)\n",
    "        # create test dataloader\n",
    "        test_loader = DataLoader(test_dataset, batch_size)\n",
    "    else:\n",
    "        test_loader = None\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition\n",
    "The model consists of 3 convolutional layers followed by a double LSTM layer. Dropout frequency has been tuned for best results, along with weight decay (L2 regularization) to keep the model from becoming too complex while still learning meaningful patterns in the limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model with three conv layers\n",
    "# The model can be set to stateful, meaning the internal hidden state and cell state is passed\n",
    "#   into the model each batch and reset once per epoch.\n",
    "class LSTM_model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, batch_size, channels):\n",
    "        super(LSTM_model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.conv_channels = channels\n",
    "\n",
    "        # ReLU activation function\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # conv dropout layer based on \"Park, S., Kwak, N. (2017). Analysis on the Dropout Effect in Convolutional Neural Networks. \n",
    "        #                              In: Lai, SH., Lepetit, V., Nishino, K., Sato, Y. (eds) Computer Vision – ACCV 2016. ACCV 2016. \n",
    "        #                              Lecture Notes in Computer Science(), vol 10112. Springer, Cham. https://doi.org/10.1007/978-3-319-54184-6_12\"\n",
    "        self.conv_dropout = nn.Dropout(0.1) \n",
    "\n",
    "        # first conv layer\n",
    "        padding = 1\n",
    "        kernel_conv2d = 2\n",
    "        c_out = channels\n",
    "        lstm_input_size = input_size - kernel_conv2d + (2 * padding) + 1\n",
    "        self.conv2d_1 = nn.Conv2d(1, c_out, kernel_size = kernel_conv2d, padding = padding)\n",
    "\n",
    "        # # second conv layer\n",
    "        padding = 1\n",
    "        kernel_conv2d += 1\n",
    "        c_out2 = c_out * 2\n",
    "        lstm_input_size = lstm_input_size - kernel_conv2d + (2 * padding) + 1\n",
    "        self.conv2d_2 = nn.Conv2d(c_out, c_out2, kernel_size = kernel_conv2d, padding = padding)\n",
    "\n",
    "        # third conv layer\n",
    "        padding = 1\n",
    "        kernel_conv2d += 1\n",
    "        c_out3 = c_out2 * 2\n",
    "        lstm_input_size = lstm_input_size - kernel_conv2d + (2 * padding) + 1\n",
    "        self.conv2d_3 = nn.Conv2d(c_out2, c_out3, kernel_size = kernel_conv2d, padding = padding)\n",
    "\n",
    "        self.lstm = nn.LSTM(c_out3 * lstm_input_size, hidden_size, num_layers, dropout=0.2, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        print(\"LSTM initialized with {} input size, {} hidden layer size, {} number of LSTM layers, and an output size of {}\".format(input_size, hidden_size, num_layers, output_size))\n",
    "        # reset states in case of stateless use\n",
    "        self.reset_states(batch_size)\n",
    "\n",
    "    # reset hidden state and cell state, should be before each new sequence\n",
    "    #   In our problem: every epoch, as it is one long sequence\n",
    "    def reset_states(self, batch_size):\n",
    "    # def reset_states(self):\n",
    "        # hidden state and cell state for LSTM \n",
    "        self.hn = torch.zeros(self.num_layers,  batch_size, self.hidden_size).to(device)\n",
    "        self.cn = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "\n",
    "    def forward(self, input, stateful):\n",
    "        # conv layer wont take it right now as it seems to have batch_size number of channels \n",
    "        # [batch_size,window_size,4]->[batch_size,1,window_size,4]\n",
    "        input = input.unsqueeze(1)\n",
    "\n",
    "        # pass through first conv layer\n",
    "        out = self.conv2d_1(input)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # dropout\n",
    "        out = self.conv_dropout(out)\n",
    "\n",
    "        # pass through second conv layer\n",
    "        out = self.conv2d_2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # dropout\n",
    "        out = self.conv_dropout(out)\n",
    "\n",
    "        # pass through third conv layer\n",
    "        out = self.conv2d_3(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        # reshape for the lstm\n",
    "        out = out.view(input.size(0), out.size(2), -1)\n",
    "\n",
    "        # simple forward function\n",
    "        # stateful = keep hidden states entire sequence length\n",
    "        if stateful:\n",
    "            # for last batch which might not be the same shape\n",
    "            if (input.size(0) != self.hn.size(1)):\n",
    "                self.reset_states(input.size(0))\n",
    "              \n",
    "            # lstm layer\n",
    "            out, (self.hn, self.cn) = self.lstm(out, (self.hn.detach(), self.cn.detach())) \n",
    "            # linear output layer\n",
    "            out = self.linear(out[:,-1,:])\n",
    "        else:\n",
    "            # initiaze hidden and cell states\n",
    "            hn = torch.zeros(self.num_layers,  input.size(0), self.hidden_size).to(device)\n",
    "            cn = torch.zeros(self.num_layers, input.size(0), self.hidden_size).to(device)\n",
    "            # lstm layer\n",
    "            out, (hn, cn) = self.lstm(out, (hn, cn))\n",
    "            # linear output layer\n",
    "            out = self.linear(out[:,-1,:])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, train_loader:DataLoader, test_loader:DataLoader, nr_epochs, optimizer, loss_func, scheduler, stateful, writer):\n",
    "    # lowest train/test loss, train/test loss lists\n",
    "    lowest_train_loss = np.inf\n",
    "    lowest_test_loss = np.inf\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # test_loss declaration untill assigned in model evaluation (used in progress bar print)\n",
    "    test_loss = \"n/a\"\n",
    "\n",
    "    # training loop\n",
    "    for epoch in (progress_bar := tqdm(range(1, nr_epochs))):\n",
    "        # add epoch info to progress bar\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        # reset lstm hidden and cell state (stateful lstm = reset states once per sequence)\n",
    "        # if not, reset automatically each forward call\n",
    "        if stateful:\n",
    "            model.reset_states(train_loader.batch_size)\n",
    "\n",
    "        # reset running loss\n",
    "        running_loss_train = 0\n",
    "        running_loss_test = 0\n",
    "\n",
    "        # train loop\n",
    "        model.train()\n",
    "        # tqdm = progress bar utility\n",
    "        # for i, (inputs, labels) in enumerate(progress_bar := tqdm(train_loader, desc = f\"Epoch {epoch}\")):\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            # reset gradient function of weights\n",
    "            optimizer.zero_grad()\n",
    "            # forward\n",
    "            prediction = model(inputs, stateful)\n",
    "            # calculate loss\n",
    "            loss = loss_func(prediction, labels)\n",
    "            # backward, retain_graph = True needed for hidden lstm states\n",
    "            loss.backward(retain_graph=True)\n",
    "            # step\n",
    "            optimizer.step()\n",
    "            # add to running loss\n",
    "            running_loss_train += loss.item()\n",
    "\n",
    "        # learning rate scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # calc running loss\n",
    "        train_loss = running_loss_train/len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # add loss to tensorboard\n",
    "        writer.add_scalar(\"Running train loss\", train_loss, epoch)        \n",
    "\n",
    "        # check if lowest loss\n",
    "        if (train_loss < lowest_train_loss):\n",
    "            lowest_train_loss = train_loss\n",
    "            # Save model\n",
    "            torch.save(model.state_dict(), \"models/model\" + str(train_loader.dataset.x.shape[1]) + str(model.hidden_size) + str(model.conv_channels) + \".pth\")\n",
    "\n",
    "        # Test evaluation\n",
    "        if (test_loader):\n",
    "            # model.eval()\n",
    "            with torch.no_grad():\n",
    "                for j, (inputs, labels) in enumerate(test_loader):\n",
    "                    # forward pass\n",
    "                    prediction = model(inputs, stateful)\n",
    "                    # calculate loss\n",
    "                    test_loss = loss_func(prediction, labels)\n",
    "                    # add to running loss\n",
    "\n",
    "            # calc running loss\n",
    "            test_loss = running_loss_test/len(test_loader)\n",
    "            test_losses.append(test_loss)\n",
    "\n",
    "            # add test loss to tensorboard\n",
    "            writer.add_scalar(\"Running test loss\", test_loss, epoch)\n",
    "\n",
    "            # if lowest till now, save model (checkpointing)\n",
    "            if (test_loss < lowest_test_loss):\n",
    "                lowest_test_loss = test_loss\n",
    "                torch.save(model.state_dict(), \"models/model\" + str(train_loader.dataset.x.shape[1]) + str(model.hidden_size) + str(model.conv_channels) + \"test\" + \".pth\")\n",
    "\n",
    "        # before next epoch: add last epoch info to progress bar\n",
    "        progress_bar.set_postfix({\"train_loss\": train_loss, \"test_loss\": test_loss})\n",
    "\n",
    "    # save hparams along with lowest train/test losses\n",
    "    writer.add_hparams(\n",
    "        {\"window_size\": train_loader.dataset.x.shape[1], \"hidden_size\": model.hidden_size, \"conv_channels\": model.conv_channels},\n",
    "        {\"MinTrainLoss\": lowest_train_loss, \"MinTestLoss\": lowest_test_loss},\n",
    "    )\n",
    "\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparamers\n",
    "The hyperparameters dictionary acts as a grid search if multiple values for the parameters are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size for training network\n",
    "batch_size = 16\n",
    "\n",
    "# split size of test/train data\n",
    "split_size = 0.0\n",
    "\n",
    "# hyperparameters for fine-tuning\n",
    "    # window_size = sliding window on time-sequence data for input\n",
    "    # hidden_size = hidden units of lstm layer(s)\n",
    "    # conv_channels = number of channels in the first conv layer (multiplied by 2 every next layer)\n",
    "    # nr_layers = number of lstm layers stacked after each other\n",
    "hyperparams = dict(\n",
    "    window_size = [80],\n",
    "    hidden_size = [256],\n",
    "    conv_channels = [8],\n",
    "    nr_layers = [2]\n",
    ")\n",
    "# sets of combinations of hparams\n",
    "hyperparam_value_sets = product(*[value for value in hyperparams.values()])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search loop\n",
    "### Lowest test loss (early stopping) and lowest train loss models get saved to *models/* under naming based on the hyperparameters.\n",
    "Tensorboard is used to see loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New run window/hidden/channels/batch_size: 48 / 16 / 8 / 16\n",
      "Input size: torch.Size([16, 48, 4]) - Output size: torch.Size([16, 98]) - TRAIN batches: 235 - TEST batches: Not available\n",
      "LSTM initialized with 4 input size, 16 hidden layer size, 2 number of LSTM layers, and an output size of 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 999/999 [3:08:34<00:00, 11.33s/it, train_loss=0.0699, test_loss=n/a]  \n"
     ]
    }
   ],
   "source": [
    "# Loop through different combinations of the hyperparameters\n",
    "for run_id, (window_size, hidden_size, conv_channels, nr_layers) in enumerate(hyperparam_value_sets):\n",
    "    # tensorboard summary writer\n",
    "    writer = SummaryWriter(f'runs/window_size={window_size} hidden_size={hidden_size} conv_channels={conv_channels}')\n",
    "    \n",
    "    # Split data in train and test, scale, create datasets and create dataloaders\n",
    "    train_loader, test_loader = createTrainTestDataloaders(voices, split_size, window_size, batch_size)\n",
    "\n",
    "    # some informational print statements\n",
    "    print(\"\\nNew run window/hidden/channels/batch_size:\", window_size, \"/\", hidden_size, \"/\", conv_channels, \"/\", batch_size)\n",
    "    features, labels = next(iter(train_loader))\n",
    "    print(\"Input size:\", features.size(), \n",
    "        \"- Output size:\", labels.size(), \n",
    "        \"- TRAIN batches:\", len(train_loader), \n",
    "        \"- TEST batches:\", len(test_loader) if test_loader else \"Not available\")\n",
    "    # Input/output dimensions\n",
    "    input_size = voices.shape[1]\n",
    "    output_size = labels.size(1)\n",
    "\n",
    "    # create model\n",
    "    lstm_model = LSTM_model(input_size, output_size, hidden_size, nr_layers, batch_size, conv_channels)\n",
    "\n",
    "    # loss function and optimizer\n",
    "    #   multi lable one hot encoded prediction only works with BCEwithlogitloss\n",
    "    loss_func = nn.BCEWithLogitsLoss()\n",
    "    # AdamW = Adam with fixed weight decay (weight decay performed after controlling parameter-wise step size)\n",
    "    optimizer = optim.AdamW(lstm_model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "    scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.5, total_iters=750)\n",
    "    \n",
    "    # to gpu if possible\n",
    "    lstm_model = lstm_model.to(device)\n",
    "    \n",
    "    # training loop\n",
    "    epochs = 1000\n",
    "    # In this example we should not use a stateful lstm, as the next samples (subsequent sliding windows) do not follow directly from the current.\n",
    "    # This is only the case when the first sample is (for Ex.) [1:10] which is the first window, and [11:20] the next, and so on.\n",
    "    # With our data it would be: [1:10] and the next [2:11]. Target value does not matter necessarily. \n",
    "    # More explanation: https://stackoverflow.com/questions/58276337/proper-way-to-feed-time-series-data-to-stateful-lstm\n",
    "    #   unfortunately I implemented stateful before knowing these in and outs.\n",
    "    stateful = False\n",
    "    train_losses, test_losses = training(lstm_model, train_loader, test_loader, epochs, optimizer, loss_func, scheduler, stateful, writer)\n",
    "\n",
    "    # flush tensorboard writer\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# quick plots\n",
    "sns.lineplot(train_losses)\n",
    "sns.lineplot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate music using one of the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to predict x next notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictNextNotes(input, steps, lstm_model, voices, scaler):\n",
    "    # predicted notes\n",
    "    predicted_notes = np.zeros((1,4))\n",
    "\n",
    "    # all unique notes for each voice\n",
    "    unique_voice1 = np.unique(voices[:,0])\n",
    "    unique_voice2 = np.unique(voices[:,1])\n",
    "    unique_voice3 = np.unique(voices[:,2])\n",
    "    unique_voice4 = np.unique(voices[:,3])\n",
    "    one_hot_values = np.concatenate((unique_voice1, unique_voice2, unique_voice3, unique_voice4))\n",
    "\n",
    "    # BCEwithLogitLoss uses sigmoid when calculating loss, but we need to pass through\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # prepare input\n",
    "    input = torch.tensor(input, dtype=torch.float32).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        for i in range(steps):\n",
    "            # print(input.shape)\n",
    "            output = lstm_model(input, stateful=False)\n",
    "            output = sigmoid(output)\n",
    "            # print(output)\n",
    "            output = output.detach().numpy().squeeze()\n",
    "\n",
    "            # get the indices with highest value from model forward output\n",
    "            note_voice1 = np.argmax(output[:len(unique_voice1)])\n",
    "            note_voice2 = np.argmax(output[len(unique_voice1) : len(unique_voice1) + len(unique_voice2)])\n",
    "            note_voice3 = np.argmax(output[len(unique_voice1) + len(unique_voice2) : len(unique_voice1) + len(unique_voice2) + len(unique_voice3)])\n",
    "            note_voice4 = np.argmax(output[-len(unique_voice4):])\n",
    "            # print(note_voice1, note_voice2, note_voice3, note_voice4)\n",
    "\n",
    "            # get notes\n",
    "            note_voice1 = one_hot_values[note_voice1]\n",
    "            note_voice2 = one_hot_values[len(unique_voice1) + note_voice2]\n",
    "            note_voice3 = one_hot_values[len(unique_voice1) + len(unique_voice2) + note_voice3]\n",
    "            note_voice4 = one_hot_values[len(unique_voice1) + len(unique_voice2) + len(unique_voice3) + note_voice4]\n",
    "\n",
    "            # add to array and inverse scale\n",
    "            next_notes = np.array([note_voice1, note_voice2, note_voice3, note_voice4])\n",
    "            next_notes_invscaled = scaler.inverse_transform(next_notes.reshape(1, -1))\n",
    "            # print(next_notes_invscaled)\n",
    "            predicted_notes = np.concatenate((predicted_notes, next_notes_invscaled), axis = 0)\n",
    "            # print(predicted_notes)\n",
    "\n",
    "            # change input\n",
    "            # drop oldest notes\n",
    "            input = input[0][1:]\n",
    "            # concat predicted notes\n",
    "            input = torch.cat((input, torch.Tensor(next_notes).unsqueeze(0)))\n",
    "            input = input.unsqueeze(0)\n",
    "\n",
    "    return(predicted_notes.astype(np.int32)[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and generate new notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "# model = LSTM_model(input_size = 4, output_size = 98, hidden_size = 64, num_layers = 2, batch_size = 32, channels = 8)\n",
    "# load model file\n",
    "# model.load_state_dict(torch.load(\"models/model80648.pth\", map_location=device))\n",
    "model = lstm_model\n",
    "\n",
    "# load data, 4 voices of instruments\n",
    "voices = np.loadtxt(\"input.txt\")\n",
    "\n",
    "# Train/test split (needed for correct scaling of new data)\n",
    "dataset_size = len(voices[:,])\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor((1 - split_size) * dataset_size))\n",
    "train_indices = indices[:split]\n",
    "# create split in data\n",
    "train_voices = voices[train_indices, :]\n",
    "\n",
    "# fit the scaler to the train data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_voices)\n",
    "# scale voices\n",
    "voices = scaler.transform(voices)\n",
    "train_voices = scaler.transform(train_voices)\n",
    "\n",
    "# take last sliding window in data and infer from there\n",
    "input = train_voices[-window_size:]\n",
    "steps = 1500\n",
    "new_music = predictNextNotes(input, steps, model, voices, scaler)\n",
    "\n",
    "# save new music\n",
    "np.savetxt(fname = \"output/output.txt\", X = new_music, fmt = \"%d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a midi files from generated music (output and complete fugue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize fluidsynth with soundfont\n",
    "fluidsynth.init(\"soundfonts/040_Florestan_String_Quartet.sf2\", \"salsa\")\n",
    "\n",
    "# load original music\n",
    "original = np.loadtxt(\"input.txt\")\n",
    "\n",
    "# load network output music\n",
    "output = np.loadtxt(\"output/output.txt\")\n",
    "\n",
    "# concatenate\n",
    "complete = np.concatenate((original, output), axis = 0)\n",
    "\n",
    "# create midi for output and comlete (original + output)\n",
    "for run, voices in enumerate([output, complete]):\n",
    "    # create 4 tracks for the 4 voices\n",
    "    encoded_voices = [Track(), Track(), Track(), Track()] \n",
    "    \n",
    "    # loop through the generated voices\n",
    "    for i, notes in enumerate([voices[:,0], voices[:,1], voices[:,2], voices[:,3]]):\n",
    "        # initialize as impossible note\n",
    "        last_note = -1\n",
    "        count = 1\n",
    "        for j, note in enumerate(notes):\n",
    "            if note:\n",
    "                if ((note == last_note) or (j == 0)):\n",
    "                    # same note as previous note\n",
    "                    count += 1\n",
    "                    last_note = note\n",
    "                    \n",
    "                    if (j + count > len(notes)):\n",
    "                        # current note reaches end of file\n",
    "                        n = Note()\n",
    "                        n.from_int(int(last_note))\n",
    "                        b = Bar()\n",
    "                        b.place_notes(n, 16/count)\n",
    "                        encoded_voices[i].add_bar(b)\n",
    "                else:\n",
    "                    # different note encountered\n",
    "                    # add previous note with its duration to track\n",
    "                    n = Note()\n",
    "                    n.from_int(int(last_note))\n",
    "                    b = Bar()\n",
    "                    \n",
    "                    # 8 should be 1/2 -> 2\n",
    "                    # 16 should be 1 -> 1\n",
    "                    # 32 should be 2 -> 0.5\n",
    "                    b.place_notes(n, duration = 16/count)\n",
    "                    encoded_voices[i].add_bar(b)\n",
    "                    \n",
    "                    # reset\n",
    "                    count = 1\n",
    "                    last_note = note\n",
    "            else:\n",
    "                # current note = 0, means a pause (silence)\n",
    "                b = Bar()\n",
    "                b.place_rest(16)\n",
    "                encoded_voices[i].add_bar(b)\n",
    "\n",
    "    output_composition = Composition()\n",
    "    output_composition.add_track(encoded_voices[0])\n",
    "    output_composition.add_track(encoded_voices[1])\n",
    "    output_composition.add_track(encoded_voices[2])\n",
    "    output_composition.add_track(encoded_voices[3])\n",
    "\n",
    "    if (run == 0):\n",
    "        midi_file_out.write_Composition(\"output/output.midi\", output_composition)\n",
    "    else:\n",
    "        midi_file_out.write_Composition(\"output/complete.midi\", output_composition)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create mp3\n",
    "For Linux.  \n",
    "windows -> remove %%bash, add !before each fluidsynth call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Playing output/output.midi\n",
      "MIDI file: output/output.midi\n",
      "Format: 1  Tracks: 4  Divisions: 72\n",
      "Sequence: Untitled\n",
      "Track name: Untitled\n",
      "Track name: Untitled\n",
      "Track name: Untitled\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "[wav @ 0x557aff2152c0] Ignoring maximum wav data size, file may be invalid\n",
      "Warning: -: Illegal seek: Can't make valid header\n",
      "Guessed Channel Layout for Input Stream #0.0 : stereo\n",
      "Input #0, wav, from 'pipe:':\n",
      "  Duration: N/A, bitrate: 1411 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> mp3 (libmp3lame))\n",
      "Output #0, mp3, to 'output/output.mp3':\n",
      "  Metadata:\n",
      "    TSSE            : Lavf58.29.100\n",
      "    Stream #0:0: Audio: mp3 (libmp3lame), 44100 Hz, stereo, s16p, 320 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libmp3lame\n",
      "Last 29 MIDI events are ignored6 bitrate= 318.0kbits/s speed=36.9x    \n",
      "Playing time: ~189 seconds\n",
      "Notes cut: 0\n",
      "Notes lost totally: 0\n",
      "pipe:: corrupt input packet in stream 0\n",
      "size=    7317kB time=00:03:07.27 bitrate= 320.1kbits/s speed=37.1x    \n",
      "video:0kB audio:7316kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.014536%\n",
      "Playing output/complete.midi\n",
      "MIDI file: output/complete.midi\n",
      "Format: 1  Tracks: 4  Divisions: 72\n",
      "Sequence: Untitled\n",
      "Track name: Untitled\n",
      "Track name: Untitled\n",
      "Track name: Untitled\n",
      "ffmpeg version 4.2.7-0ubuntu0.1 Copyright (c) 2000-2022 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.4.0-1ubuntu1~20.04.1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "[wav @ 0x55d8e341c2c0] Ignoring maximum wav data size, file may be invalid\n",
      "Warning: -: Illegal seek: Can't make valid header\n",
      "Guessed Channel Layout for Input Stream #0.0 : stereo\n",
      "Input #0, wav, from 'pipe:':\n",
      "  Duration: N/A, bitrate: 1411 kb/s\n",
      "    Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (pcm_s16le (native) -> mp3 (libmp3lame))\n",
      "Output #0, mp3, to 'output/complete.mp3':\n",
      "  Metadata:\n",
      "    TSSE            : Lavf58.29.100\n",
      "    Stream #0:0: Audio: mp3 (libmp3lame), 44100 Hz, stereo, s16p, 320 kb/s\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libmp3lame\n",
      "Last 29 MIDI events are ignored5 bitrate= 320.0kbits/s speed=31.8x    \n",
      "Playing time: ~645 seconds\n",
      "Notes cut: 0\n",
      "Notes lost totally: 0\n",
      "pipe:: corrupt input packet in stream 0\n",
      "size=   25141kB time=00:10:43.55 bitrate= 320.0kbits/s speed=32.4x    \n",
      "video:0kB audio:25140kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.004230%\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "timidity output/output.midi -Ow -o - | ffmpeg -i - -acodec libmp3lame -ab 320k output/output.mp3\n",
    "timidity output/complete.midi -Ow -o - | ffmpeg -i - -acodec libmp3lame -ab 320k output/complete.mp3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
